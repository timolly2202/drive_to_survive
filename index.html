<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Drive to Survive Portfolio for 41118 AI in robotics class at UTS.">
  <meta name="keywords" content="41118">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Drive to Survive</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">41118 Portfolio - Drive to Survive</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">13886965 - Tejas Bhuta</a>,
            </span>
            <span class="author-block">
              <a href="#">13886965 - Jarred Deluca</a>
            </span>
            
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">13892512 - Tim Ollerton</a>,
            </span>
            <span class="author-block">
              <a href="#">13288232 - Wil Coxon</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University Technology Sydney</span>
            <!-- <span class="author-block"><sup>2</sup>Other institute</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. 
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#portfolio-video"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Portfolio Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/timolly2202/drive_to_survive"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/rFalque/41118_project-sample/tree/main/data_sample"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png"
            class="teaser-image"
            alt="Teaser."/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Drive to Survive</span> 
        sample of teaser image for your project. 
        Note that this image is AI generated and does not reflect any flowchart of an 
        actual framework. However, this text would be a great place to give a quick 
        summary of your own flowchart.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Team members</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="./static/images/staff/Raphael_Falque.jpg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-chair-tp">
          <img src="./static/images/staff/Lan_Wu.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-fullbody">
          <img src="./static/images/staff/Hugh_Byun.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/staff/Dominik_Slomma.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-blueshirt">
          <img src="./static/images/staff/Augustine_Le.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-mask">
          <img src="./static/images/staff/Resul_Dagdanov.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Drive to Survive is a project that aims to create an autonomous
            driving system that can navigate through a simulated environment
            using reinforcement learning and computer vision techniques. The
            project explores the use of YOLO for object detection,
            LiDAR for environment mapping, and reinforcement learning for
            decision-making. The goal is to develop a system that can learn
            to drive safely and efficiently in a dynamic environment, avoiding
            obstacles and making decisions based on the detected objects.
          </p>
          <p>
            The project is implemented in ROS2 (Humble) and uses Gazebo for
            simulation. The system is designed to be modular, allowing for easy
            integration of new components and algorithms. The project also
            includes a portfolio video that demonstrates the system in action,
            showcasing the capabilities of the autonomous driving system.
          </p>
          <p>
            The video below explain how to set up your portfolio.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Portfolio video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 id="portfolio-video" class="title is-3">Portfolio Video SAMPLE STILL</h2>
        <!-- 
        <p>
          As a youtube link:
        </p>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        -->
        <!-- <p>
          Directly embedded video:
        </p> -->
        <div class="content has-text-centered">
          <video id="replay-video"
                  controls
                  preload
                  playsinline
                  width="100%">
            <source src="./static/videos/portfolio_instructions.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Portfolio video. -->
    
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop content">
  <!-- Setup. -->
  <h2 class="title is-3" id="project-setup">Project Setup</h2>

    <p>This project requires ROS2 (Humble) with Gazebo and RViz, set up on Ubuntu 22.04 in order to run.</p>

    <pre><code class="language-bash">
    sudo apt install ros-$ROS_DISTRO-tf-transformations
    pip install numpy==1.23.5 scikit-learn pandas joblib pytorch ultralytics cv-bridge
    </code></pre>

    <p>Once these dependencies are installed, clone the repository into your workspace and install rosdep:</p>

    <ol>
      <li><strong>Clone the repository into your workspace:</strong>
        <pre><code class="language-bash">
    git clone git@github.com:timolly2202/drive_to_survive.git
        </code></pre>
      </li>

      <li><strong>Install Dependencies</strong>
        <pre><code class="language-bash">
    rosdep install --from-paths src --ignore-src -r -y
        </code></pre>
      </li>

      <li><strong>Build Packages</strong>
        <pre><code class="language-bash">
    cd ~/drive_to_survive
    colcon build --symlink-install
    source install/setup.bash
        </code></pre>
      </li>
    </ol>
   
  <!-- Setup. -->

  <!-- Running the code -->
   <h2 class="title is-3" id="project-setup">Running the Project</h2>

    <p>This project requires ROS2 (Humble) with Gazebo and RViz, set up on Ubuntu 22.04 in order to run.</p>

    <pre><code class="language-bash">
    # Launch world
    ros2 launch gazebo_tf drive_to_survive.launch.py

    # Start YOLO detectors
    ros2 launch audibot_yolo multi_camera.launch.py

    # Run RL Rewards
    ros2 run rl_rewards rl_rewards

    # Start RL Agent
    ros2 run audibot_rl_driver rl_agent_node
    </code></pre>

  <!-- Running the code -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns">
  
      <!-- Environment -->
      <div class="column is-half">
        <div class="content">
          <h2 class="title is-3">Project Environment</h2>
          <p>The project environment is a simulated world created using Gazebo, which includes various objects such as cones and firetrucks. The environment is designed to test the autonomous driving system's ability to navigate and interact with these objects.</p>
          <p>
            The environment is set up with a 2D LiDAR sensor and multiple RGB-D cameras mounted on the Audi, which provide data for object detection and classification. The cameras are used to detect and classify objects using YOLO.
          </p>
          
          <figure>
            <img src="./static/images/environment.png"
                class="teaser-image"
                alt="Project Environment">
          </figure>

          <figure>
            <img src="./static/images/environment2.png"
                class="teaser-image"
                alt="Environment 2 Image">
            <figcaption class="has-text-centered mt-2"><em>Environment 2 Image</em></figcaption>
          </figure>

        </div>
      </div>
      
      <!-- Lidar Processing -->
      <div class="column is-half">
        <div class="content">
          <h2 class="title is-3">Lidar Processing</h2>
          <p>
            <strong>cluster_puck</strong> is a ROS 2 package for detecting, classifying, and visualizing clustered objects (e.g., cones) from 2D LiDAR and odometry data. It estimates clusters via spatial gaps, applies KMeans and PCA, and classifies them using an SVM. Confirmed cones are tracked live in a dynamic ConeMap and visualized in RViz using green scan points, blue cluster centers, purple PCA ellipsoids, and orange cones. It supports real-time labeling, training, and export, and is extensible for planning integration.
          </p>
          <video class="teaser-image" controls>
            <source src="./static/videos/LidarConeDiscoveryMode.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p><em>Video 1: Cluster detection and PCA ellipsoids in discovery-mode</em></p>
          <video class="teaser-image" controls>
            <source src="./static/videos/SVMConeDetection.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p><em>Video 2: SVM-based cone classification and ConeMap visualization in real-time</em></p>
        </div>
      </div>
    
    </div>       
    
    <div class="columns is-centered">
      <!-- YOLO. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">YOLO detection of cones</h2>
          <p>
            Images from the environment were taken and manually marked with cone and firetruck 
            bounding boxes in order to train the YOLO detection system for the 4 cameras mounted
            on the Audi. The below video shows the detection system in action.
          </p>

            <video id="replay-video"
                  controls
                  preload
                  playsinline
                  width="100%">
            <source src="./static/videos/audi_moving_yolo.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ YOLO. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">RL Training</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              The project was trained use a DQN model and a discrete action space. 
              The graph below shows our training rewards over the episodes, which jumped over the place, 
              which indicated that we needed to change our reward function for more consistent learning. 
              The car was able to drive towards goals, however it crashed into the cones which affected how the 
              car moved.
            </p>
            <img src="./static/images/dqn_training_rewards.png"
            class="teaser-image"
            alt="Teaser."/>
          </div>
        </div>
      </div>
      <!--/ Matting. -->
    </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results discussion and reflection</h2>

        <div class="content has-text-justified">
          <p>
            This is a sample text to show how the results discussion and 
            reflection would look like in your own project. You can use this 
            space to describe the main results obtained, the limitations of your work, 
            and the future work that could be done.
          </p>
          <p>
            Orem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod 
            tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, 
            quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo 
            consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse 
            cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat 
            non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
          </p>
          <p>
            Tim's dot points:
            <p>Challenges with RL</p>
            <ul>
              <li>
                We had trouble getting the gazebo to reset after each episode. It ended up needing a full shutdown and restart with subprocesses which caused problems.
                <ul>
                  <li>Subprocesses didn't work nicely with gzserver and took a while to figure out.</li>
                </ul>
              </li>
              <li>
                We started out going for just goals in training, which caused it to crash into the cones. It also stopped since we didn't give it less rewards for staying still. This led to erratic training rewards, as seen in the DQN training graph.
              </li>
              <li>
                We didn't have time to make the rewards system more expansive (like staying in track and not hitting cones, etc) so this is what we would do in the future.
              </li>
            </ul>

            <p>YOLO Successes</p>
            <ul>
              <li>
                Had to manually label 200 images to train the YOLO to detect cones and firetrucks so we could put it in the observation space for the car. It worked really well after training at getting the objects relatively close to the car. It was less successful for the cones further away since in some cases they were only around 4 pixels.
              </li>
              <li>
                All four cameras get all the close cones bounding boxes surrounding the car.
              </li>
            </ul>
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="references">
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>

    <p>ElevenLabs. (n.d.). <em>Speech synthesis</em>. ElevenLabs. <a href="https://elevenlabs.io/app/speech-synthesis/text-to-speech" target="_blank">https://elevenlabs.io/app/speech-synthesis/text-to-speech</a></p>

    <p>Open Robotics. (n.d.). <em>rospy</em>. ROS Wiki. <a href="http://wiki.ros.org/rospy" target="_blank">http://wiki.ros.org/rospy</a></p>

    <p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... Chintala, S. (2019). <em>PyTorch: An imperative style, high-performance deep learning library</em>. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, & R. Garnett (Eds.), <em>Advances in Neural Information Processing Systems, 32</em>, 8024–8035. Curran Associates, Inc. <a href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" target="_blank">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a></p>

    <p>Python Software Foundation. (2024). <em>Python 3.13.0 documentation</em>. <a href="https://docs.python.org/3/" target="_blank">https://docs.python.org/3/</a></p>

    <p>Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). <em>You Only Look Once: Unified, Real-Time Object Detection</em>. In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> (pp. 779–788). IEEE. <a href="https://doi.org/10.1109/CVPR.2016.91" target="_blank">https://doi.org/10.1109/CVPR.2016.91</a></p>

    <p>ROS 2 Project. (n.d.). <em>Gazebo simulation for ROS 2</em>. Open Robotics. <a href="https://gazebosim.org/docs/latest/ros2_overview" target="_blank">https://gazebosim.org/docs/latest/ros2_overview</a></p>

    <p>Scikit-learn Developers. (n.d.). <em>About us</em>. scikit-learn. <a href="https://scikit-learn.org/stable/about.html" target="_blank">https://scikit-learn.org/stable/about.html</a></p>

    <p>Robustify. (n.d.). <em>audibot</em>. GitHub. <a href="https://github.com/robustify/audibot" target="_blank">https://github.com/robustify/audibot</a></p>

  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
