<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Drive to Survive Portfolio for 41118 AI in robotics class at UTS.">
  <meta name="keywords" content="41118">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Drive to Survive</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">41118 Portfolio - Drive to Survive</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">13886965 - Tejas Bhuta</a>,
            </span>
            <span class="author-block">
              <a href="#">13886965 - Jarred Deluca</a>
            </span>
            
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">13892512 - Tim Ollerton</a>,
            </span>
            <span class="author-block">
              <a href="#">13288232 - Wil Coxon</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University Technology Sydney</span>
            <!-- <span class="author-block"><sup>2</sup>Other institute</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. 
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#portfolio-video"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Portfolio Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/timolly2202/drive_to_survive"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/rFalque/41118_project-sample/tree/main/data_sample"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png"
            class="teaser-image"
            alt="Teaser."/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Drive to Survive</span> 
        Here are the final main components of the system:
        <br><br>
        -audibot_rl_driver (DQN) ROS node:
        Controls the Audibot using a Deep Q-Network, manages goal navigation, processes observations, calculates rewards, and refines learning through hyperparameter adjustments and an epsilon-greedy strategy.
        <br><br>
        -rl_gym ROS node:
        Serves as the primary environment launcher and resetter for the Gazebo simulation during training episodes.
        <br><br>
        -Gazebo environment:
        A simulated world featuring a race track, various objects (cones, firetrucks), and an Audi R8 model, equipped with a 2D LiDAR sensor and RGB-D cameras.
        <br><br>
        -YOLO object detection system:
        A CNN-based system trained to identify cones and firetrucks within the Gazebo environment using manually labeled images from the Audi's cameras.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Team members</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="./static/images/staff/Raphael_Falque.jpg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-chair-tp">
          <img src="./static/images/staff/Lan_Wu.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-fullbody">
          <img src="./static/images/staff/Hugh_Byun.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/staff/Dominik_Slomma.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-blueshirt">
          <img src="./static/images/staff/Augustine_Le.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
        <div class="item item-mask">
          <img src="./static/images/staff/Resul_Dagdanov.jpeg"
            class="teaser-image"
            alt="Teaser."/>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Drive to Survive is a project that aims to create an autonomous
            driving system that can navigate through a simulated environment
            using reinforcement learning and computer vision techniques. The
            project explores the use of YOLO for object detection,
            LiDAR for environment mapping, and reinforcement learning for
            decision-making. The goal is to develop a system that can learn
            to drive safely and efficiently in a dynamic environment, avoiding
            obstacles and making decisions based on the detected objects.
          </p>
          <p>
            The project is implemented in ROS2 (Humble) and uses Gazebo for
            simulation. The system is designed to be modular, allowing for easy
            integration of new components and algorithms. The project also
            includes a portfolio video that demonstrates the system in action,
            showcasing the capabilities of the autonomous driving system.
          </p>
          <p>
            The video below explain how to set up your portfolio.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Portfolio video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 id="portfolio-video" class="title is-3">Drive_2_Survive Project Video</h2>
        <!-- 
        <p>
          As a youtube link:
        </p>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        -->
        <!-- <p>
          Directly embedded video:
        </p> -->
        <div class="publication-video">
          <iframe src="https://www.youtube.com/watch?v=_IHY9wzep7Y&ab_channel=WilliamCoxon"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Portfolio video. -->
    
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop content">
  <!-- Setup. -->
  <h2 class="title is-3" id="project-setup">Project Setup</h2>

    <p>This project requires ROS2 (Humble) with Gazebo and RViz, set up on Ubuntu 22.04 in order to run.</p>

    <pre><code class="language-bash">
    sudo apt install ros-$ROS_DISTRO-tf-transformations
    pip install numpy==1.23.5 scikit-learn pandas joblib pytorch ultralytics cv-bridge
    </code></pre>

    <p>Once these dependencies are installed, clone the repository into your workspace and install rosdep:</p>

    <ol>
      <li><strong>Clone the repository into your workspace:</strong>
        <pre><code class="language-bash">
    git clone git@github.com:timolly2202/drive_to_survive.git
        </code></pre>
      </li>

      <li><strong>Install Dependencies</strong>
        <pre><code class="language-bash">
    rosdep install --from-paths src --ignore-src -r -y
        </code></pre>
      </li>

      <li><strong>Build Packages</strong>
        <pre><code class="language-bash">
    cd ~/drive_to_survive
    colcon build --symlink-install
    source install/setup.bash
        </code></pre>
      </li>
    </ol>
   
  <!-- Setup. -->

<!-- Running the code -->
<h2 class="title is-3" id="project-setup">Running the Project</h2>

<p>This project requires ROS2 (Humble) with Gazebo and RViz, set up on Ubuntu 22.04 in order to run.</p>

<p>Once the environment is built, launch the following components from separate terminals:</p>

<pre><code class="language-bash">
# 1. Launch the environment manager (with optional RViz visualization)
ros2 run rl_gym environment_manager use_rviz:=true

# 2. In a new terminal, start the RL agent
ros2 run audibot_rl_driver rl_agent_node
</code></pre>

<p>Optionally, you can run <code>use_rviz:=false</code> if RViz visualization is not needed.</p>
<p>More detailed setup and usage instructions are available on the GitHub repository:
<a href="https://github.com/timolly2202/drive_to_survive" target="_blank">https://github.com/timolly2202/drive_to_survive</a></p>
<!--/ Running the code -->


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns">
  
      <!-- Environment -->
      <div class="column is-half">
        <div class="content">
          <h2 class="title is-3">Project Environment</h2>
          <p>The project environment is a simulated world created using Gazebo, which includes various objects such as cones and firetrucks. The environment is designed to test the autonomous driving system's ability to navigate and interact with these objects. The environment is set up with a 2D LiDAR sensor and multiple RGB-D cameras mounted on the Audi, which provide data for object detection and classification. The cameras are used to detect and classify objects using YOLO.
          </p>
          
          <figure>
            <img src="./static/images/environment3.png"
                class="teaser-image"
                alt="Project Environment">
          </figure>

          <figure>
            <img src="./static/images/environment2.png"
                class="teaser-image"
                alt="Environment 2 Image">
            <figcaption class="has-text-centered mt-2"><em>Environment Images</em></figcaption>
          </figure>

        </div>
      </div>
      
      <!-- Lidar Processing -->
      <div class="column is-half">
        <div class="content">
          <h2 class="title is-3">Lidar Processing</h2>
          <p>
            <strong>cluster_puck</strong> is a ROS 2 package for detecting, classifying, and visualizing clustered objects (e.g., cones) from 2D LiDAR and odometry data. It estimates clusters via spatial gaps, applies KMeans and PCA, and classifies them using an SVM. Confirmed cones are tracked live in a dynamic ConeMap and visualized in RViz using green scan points, blue cluster centers, purple PCA ellipsoids, and orange cones. It supports real-time labeling, training, and export, and is extensible for planning integration.
          </p>
          <video class="teaser-image" controls>
            <source src="./static/videos/LidarConeDiscoveryMode.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p><em>Video 1: Cluster detection and PCA ellipsoids in discovery-mode</em></p>
          <video class="teaser-image" controls>
            <source src="./static/videos/SVMConeDetection.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p><em>Video 2: SVM-based cone classification and ConeMap visualization in real-time</em></p>
        </div>
      </div>
    
    </div>       
    
    <div class="columns is-centered">
      <!-- YOLO. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">YOLO detection of cones</h2>
          <p>
            Images from the environment were taken and manually marked with cone and firetruck 
            bounding boxes in order to train the YOLO detection system for the 4 cameras mounted
            on the Audi. The below video shows the detection system in action.
          </p>

            <video id="replay-video"
                  controls
                  preload
                  playsinline
                  width="100%">
            <source src="./static/videos/audi_moving_yolo.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ YOLO. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">RL Training</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              The project was trained use a DQN model and a discrete action space. 
              The graph below shows our training rewards over the episodes, which jumped over the place, 
              which indicated that we needed to change our reward function for more consistent learning. 
              The car was able to drive towards goals, however it crashed into the cones which affected how the 
              car moved.
            </p>
            <img src="./static/images/dqn_training_rewards.png"
            class="teaser-image"
            alt="Teaser."/>
          </div>
        </div>
      </div>
      <!--/ Matting. -->
    </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results, Discussion and Reflection</h2>

        <div class="content has-text-justified">
          <div class="content has-text-justified">
          <h3>1. Introduction to Results and Discussion</h3>
          <p>
            This section outlines the outcomes of the "Drive to Survive" project. The final aim was to develop a DQN-based autonomous agent capable of sequential goal navigation within a simulated Gazebo environment, using YOLO for cone perception. We will cover the results achieved, the evolution of the project including challenges faced, the adaptations made by the team, and a reflection on the overall process and learnings.
          </p>

          <h3>2. Achieved Results</h3>
          <h4>Created a Gazebo Environment and ROS 2 Integration</h4>
          <p>
            A simulated driving environment was configured in Gazebo, adapted from resources used in a previous "Programming for Mechatronic Systems" subject. This environment leverages ROS 2 (Humble), providing a relevant platform for developing and testing AI algorithms in a robotics context, aligning well with the "Artificial Intelligence in Robotics" class objectives.
          </p>
          <h4>Trained YOLO Object Detection</h4>
          <p>
            A YOLO (You Only Look Once) object detection system based on Convolutional Neural Networks (CNN) was trained to identify cones and firetrucks. This involved manually labeling approximately 200 images from the simulation to create a custom dataset. The trained model demonstrated good performance in detecting objects relatively close to the car across all four of its simulated cameras. While detection of distant cones (appearing as only a few pixels) was less reliable, the system effectively provided bounding boxes for nearby cones.
          </p>
          <h4>rl_gym Environment Management for training episodes</h4>
          <p>
            To facilitate Reinforcement Learning training, a dedicated ROS 2 package named rl_gym was developed. Its primary role was to manage the Gazebo simulation lifecycle for each training episode. Upon receiving an episode start request from the DQN agent, the rl_gym node would launch the Gazebo world and the necessary camera nodes. Once the environment was operational, it published a confirmation signal back to the DQN agent. After the Rewards Package signaled the end of an episode (by publishing the final rewards), rl_gym was designed to terminate these nodes and await the next training episode request.
          </p>
          <h4>Trained a DQN RL Agent</h4>
          <p>
            A DQN agent was implemented to navigate the simulated environment by driving towards a sequence of predefined goals. The agent utilized a discrete action space, mapping its learned policy to throttle, steering, and brake commands for the simulated Audibot. During training over 1300 episodes, the agent demonstrated the ability to drive towards these goals. However, initial training, which focused solely on reaching goals, sometimes resulted in collisions with cones and periods of inaction due to an incomplete reward function. The training reward graph indicated fluctuating performance, highlighting the need for further refinement of the reward signals for more consistent learning.
          </p>
          <h4>Setup a LIDAR-Based Cone Classification with SVM</h4>
          <p>
            A LIDAR processing pipeline was developed using a ROS 2 package named cluster_puck. This system could detect object clusters from 2D LIDAR scans, apply KMeans clustering and Principal Component Analysis (PCA) for feature extraction, and then classify these clusters (e.g., as cones) using a Support Vector Machine (SVM). This component successfully demonstrated real-time labeling, training, and visualization of detected cones in RViz.
          </p>
          <h4>Made an Experimental Rewards setup (rl_rewards_node)</h4>
          <p>
            A separate ROS 2 node, rl_rewards_node, was created to enhance modularity and better integrate with the ROS 2 ecosystem. The idea was that it would act as an "episode supervisor." This node was responsible for Loading and publishing the sequence of target goals. Determining if the car was on the track using YOLO detections. Detecting episode termination conditions (e.g., off-track, task completion, timeout). Calculating and publishing a final cumulative reward for the episode.
          </p>

          <h3>3. Discussion of Challenges and Project Evolution</h3>
          <p>
            The project initially aimed for a comprehensive autonomous driving system within the ROS 2 and Gazebo framework. We explored using the Reinforcement Learning algorithm, Proxy Policy Optimisation (PPO), to train the Audibot. This was envisioned to be supported by multiple, interconnected ROS 2 modules: YOLO for cone perception, LIDAR processing, an external rl_rewards_node for managing game logic and scoring, and an rl_gym package to interface with the Gazebo environment. The goal was a modular design, allowing different team members to develop and contribute each component as a ROS Node.
          </p>
          <p>
            While each package was completed successfully on its own, several challenges arose when integrating them.
          </p>
          <p>
            Our initial project design involved two ROS 2 nodes for perceiving cones for the agent's observation space. We planned to use information from two distinct systems: camera-based YOLO object detection and the LIDAR-based cluster_puck package. The yolo was designed to identify the cone (from the fire truck) while the lidar calculated its position from the car. This may have been an overly complex setup. After the mid-project presentation we received the following feedback from Raphael Falque, who noted, "... You probably want to drop one of the implementations for the cone detection or combine them (e.g., yolo for cone detection and lidar for position prediction)". We tools this advice onboard and continued on with just Yolo in the observation space in the meantime to streamline getting the RL training.
          </p>
          <p>
            Another challenge we faced was reliably managing the Gazebo simulation for repeated RL training episodes. We encountered issues with consistently resetting the environment after each episode, which initially required full simulation shutdowns and restarts via subprocesses. These subprocesses did not always interact smoothly with the gzserver, and resolving these issues took considerable time. Adapting the pre-existing Gazebo environment from a prior subject also involved a significant setup effort to meet the specific demands of an RL training pipeline.
          </p>
          <p>
            Early attempts at RL training faced challenges in achieving stable learning. When the training focused solely on reaching waypoints, the agent often collided with cones or would remain stationary, as the initial reward structure did not sufficiently penalize inaction or encourage path following. This resulted in erratic training reward patterns. The initial design, which included the external rl_rewards_node, also meant the RL agent received its primary feedback through this separate module, which may have introduced delays or complexities in the reward signaling necessary for effective step-by-step learning.
          </p>
          <p>
            The turning point in the project was when we decided to set up a Deep Q-Network (DQN) for your training agent. This decision to ditch PPO was partly influenced by prior positive experiences with DQN, providing a more familiar framework. The primary objective was to train the AudiBot to drive sequentially from goal to goal. We streamlined the code by absorbing the Dense Reward and Goal Management into the logic within the audibot_rl_driver (DQN) node. This significantly improved reliability and consistency of the episodes. The DQN agent was now responsible for calculating its own dense, step-by-step rewards. It achieved this by directly subscribing to and interpreting signals like /in_track_status and /current_goal from Jarred's rl_rewards_node, rather than relying on a fully externalized dense reward calculation. Furthermore, the DQN node took over the management of progressing through the sequence of goals, including loading the goals.json file, tracking the current goal_index, and checking for goal achievement using logic similar to that in the original rl_rewards_node.
          </p>
          <p>
            What we didn’t realize at the time of making the separate rl_rewards package was that the DQN agent needs to consistently receive rewards, step by step, during the episode. We initially thought it just received the total rewards at the end of the episode. This was a learning curve for us.
          </p>
          <p>
            Next, we simplified the The rl_gym package's function and made it the primary environment launcher and resetter. The DQN node (audibot_rl_driver) assumed responsibility for direct sensor subscriptions and all observation processing. Now that we had it training continually we could start turning the DQN hyperparameters. They were iteratively adjusted to improve learning. Parameters such as the LEARNING_RATE for the AdamW optimizer, the EPS_DECAY rate governing the exploration-exploitation balance, and the TARGET_UPDATE_FREQUENCY for the target Q-network were refined based on observed training performance and agent stability during training sessions.
          </p>
          <p>
            The agent was set up with an epsilon-greedy strategy for action selection. Initially, a high epsilon value meant broad exploration of the discrete action space by choosing random actions. This epsilon gradually decayed over total_agent_steps_taken, prompting the agent to start to exploit its learned Q-values by selecting actions predicted to have the highest reward. To further guide the agent in the early training phases when the policy was still largely random, a heuristic-assisted action selection mechanism was implemented. With a decaying probability (controlled by ASSIST_PROB_START, ASSIST_PROB_END, and ASSIST_DECAY_STEPS parameters), the agent would temporarily use a simple heuristic to choose an action aimed at steering towards the current goal. This was designed to accelerate initial learning by guiding the agent towards more relevant state-action regions.
          </p>
          <p>
            As a result of these adaptations and dedicated training over several days, the DQN agent began to show a slight positive learning trend. The cumulative rewards per episode, though initially negative or highly variable, started to trend towards positive values over approximately 1300 training episodes, however it wasn't very stable with alot of spikes. This indicated that the agent might have need more turing in the rewards logic to navigate towards the goals more effectively and incur fewer penalties. While challenges like occasional cone collisions persisted due to the reward function's primary focus on goal achievement, the overall trajectory demonstrated satisfactory learning within the simplified reward framework.
          </p>

          <h3>4. Reflection and Lessons Learned</h3>
          <p>
            The biggest lesson we learned from this project was to start simple and build up gradually, rather than developing multiple systems and trying to integrate everything at once. Simplifying the architecture by consolidating everything into the DQN agent node (audibot_rl_driver) made a big difference, ensuring the agent received immediate and consistent feedback. However, we’re glad we experimented with nodes, it showed real potential for modular ROS2 reinforcement learning projects in the future.
          </p>
          <p>
            Training YOLO to detect cones and firetrucks showed us that a lot of the work in AI happens before the model even runs. One teammate had to manually label around 200 images, which took time but was key to getting good results. Good Data collection and preparation is just as important as the algorithm itself.
          </p>
          <p>
            This project highlighted the value of an iterative approach and strong teamwork. Our early attempt with PPO evolved into a more practical DQN implementation, aligning better with our skills and timeline. Setting up a working Gazebo training environment also demanded persistence, especially in solving reset and launch issues. Regular online meetings kept communication clear and helped us work through challenges together. This collaborative process was key to getting the project over the line.
          </p>

          <h3>5. Future recommendations</h3>
          <p>
            We can improve the dense reward function by adding small penalties for hitting cones or going off track. We could also discourage jerky or aggressive moves by giving negative rewards for sudden steering or acceleration. This would help the car learn to drive more smoothly. From there we would hopefully see more stable positive rewards on the graph after training.
          </p>

          <h3>6. Conclusion</h3>
          <p>
            The project successfully pivoted to develop a DQN agent capable of sequential goal navigation in a ROS 2 Gazebo environment, utilizing YOLO for cone perception. The agent demonstrated a positive learning trend over approximately 1300 training episodes. Key lessons from this project include the value of iterative development, realistic scope management, the critical need for dense reward signals in RL, and the practical effort involved in data preparation for perception systems like YOLO. The team's adaptability was crucial in delivering a functional RL-controlled agent.
          </p>
        </div>
          
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="references">
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>

    <p>ElevenLabs. (n.d.). <em>Speech synthesis</em>. ElevenLabs. <a href="https://elevenlabs.io/app/speech-synthesis/text-to-speech" target="_blank">https://elevenlabs.io/app/speech-synthesis/text-to-speech</a></p>

    <p>Open Robotics. (n.d.). <em>rospy</em>. ROS Wiki. <a href="http://wiki.ros.org/rospy" target="_blank">http://wiki.ros.org/rospy</a></p>

    <p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... Chintala, S. (2019). <em>PyTorch: An imperative style, high-performance deep learning library</em>. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, & R. Garnett (Eds.), <em>Advances in Neural Information Processing Systems, 32</em>, 8024–8035. Curran Associates, Inc. <a href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" target="_blank">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a></p>

    <p>Python Software Foundation. (2024). <em>Python 3.13.0 documentation</em>. <a href="https://docs.python.org/3/" target="_blank">https://docs.python.org/3/</a></p>

    <p>Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). <em>You Only Look Once: Unified, Real-Time Object Detection</em>. In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> (pp. 779–788). IEEE. <a href="https://doi.org/10.1109/CVPR.2016.91" target="_blank">https://doi.org/10.1109/CVPR.2016.91</a></p>

    <p>ROS 2 Project. (n.d.). <em>Gazebo simulation for ROS 2</em>. Open Robotics. <a href="https://gazebosim.org/docs/latest/ros2_overview" target="_blank">https://gazebosim.org/docs/latest/ros2_overview</a></p>

    <p>Scikit-learn Developers. (n.d.). <em>About us</em>. scikit-learn. <a href="https://scikit-learn.org/stable/about.html" target="_blank">https://scikit-learn.org/stable/about.html</a></p>

    <p>Robustify. (n.d.). <em>audibot</em>. GitHub. <a href="https://github.com/robustify/audibot" target="_blank">https://github.com/robustify/audibot</a></p>

  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
